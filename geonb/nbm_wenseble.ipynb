{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import sys \n",
    "from keyvars import ufiles_path\n",
    "sys.path.append(ufiles_path)\n",
    "\n",
    "import uvars\n",
    "import uviz\n",
    "import uerrortab as uterror\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "paths = uvars.parqts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/c/Users/Joseph/Documents/phd/datasets/TILEex/idata/sample_patches_parqt/sample_patches_parqt_9idx.parquet',\n",
       " '/mnt/c/Users/Joseph/Documents/phd/datasets/TILEex/idata/sample_patches_parqt/sample_patches_parqt_15idx.parquet',\n",
       " '/mnt/c/Users/Joseph/Documents/phd/datasets/TILEex/idata/sample_patches_parqt/sample_patches_parqt_25idx.parquet',\n",
       " '/mnt/c/Users/Joseph/Documents/phd/datasets/TILEex/idata/sample_patches_parqt/sample_patches_parqt_300idx.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = uvars.parqts\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "demcols = ['cop', 'edem', 'pband','tdemx']\n",
    "ftcols = ['cop', 'edem', 'pband', 's1', 'tdemx', 'wc', 'wc_ffth',\n",
    "       'wc_gau', 'wc_sobelm', 'wc_sobelh', 'wc_sobelv']\n",
    "\n",
    "fcolx = ['tdemx','edem', 'wc_ffth','wc_gau', 'wc_sobelm', 'wc_sobelh', 'wc_sobelv']\n",
    "tcolx = 'pband'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: use my iter cb pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,GradientBoostingRegressor, \n",
    "    VotingRegressor,StackingRegressor)\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('rf',RandomForestRegressor()),\n",
    "    ('gb',GradientBoostingRegressor()),\n",
    "    ('br',BayesianRidge())\n",
    "]\n",
    "\n",
    "weights = [0.7, 0.9, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingRegressor(estimators=models, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65536, 7), (65536,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[fcolx].values \n",
    "y = df[tcolx].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (30736, 7) (30736,)\n",
      "valid: (15139, 7) (15139,)\n",
      "test :(45875, 7) (45875,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "# split the full train set into train and validation sets\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=1)\n",
    "\n",
    "print(f'train: {xtrain.shape} {ytrain.shape}')\n",
    "print(f'valid: {xvalid.shape} {yvalid.shape}')\n",
    "print(f'test :{Xtrain.shape} {Ytrain.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models.append()\n",
    "    models.append(('rf',RandomForestRegressor()))\n",
    "    models.append(('gb',GradientBoostingRegressor()))\n",
    "    models.append(('br',BayesianRidge()))\n",
    "    return models\n",
    "\n",
    "def transform_rmse_to_percentage(rmse, max_rmse):\n",
    "    percentage_rmse = (1 - rmse / max_rmse) #* 100\n",
    "    return percentage_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, xtrain, xvalid, ytrain, yvalid, T=False):\n",
    "    scores = []\n",
    "    for name, model in models:\n",
    "        model.fit(xtrain,ytrain)\n",
    "        pvalid = model.predict(xvalid)\n",
    "        error = uterror.get_tab_metrics(yvalid, pvalid)\n",
    "        #print(error)\n",
    "        R2,ME, RMSE, MAD, LE90, EVS = error\n",
    "        ptc = -RMSE\n",
    "        if T:\n",
    "            ptc = transform_rmse_to_percentage(RMSE, 100)\n",
    "        \n",
    "        print(f'{name} {RMSE} {ptc}')\n",
    "        # define accuracy metric for regression that does not go banana like R2\n",
    "      \n",
    "        scores.append(ptc)\n",
    "    return scores\n",
    "\n",
    "def evaluate_ensemble(ensemble, x1,x2, y1,y2):\n",
    "    ensemble.fit(x1,y1)\n",
    "    p2 = ensemble.predict(x2)\n",
    "    error = uterror.get_tab_metrics(y2, p2)\n",
    "    R2,ME, RMSE, MAD, LE90, EVS = error\n",
    "    ptc = transform_rmse_to_percentage(RMSE, 100)\n",
    "    print(f'RMSE:{RMSE} {ptc}% \\nR2:{R2} EVS:{EVS} MBE:{ME} MAD:{MAD} L90:{LE90}')\n",
    "    #if pct:\n",
    "        #ptc = transform_rmse_to_percentage(RMSE, 100)\n",
    "        #print(f'RMSE val:{RMSE} pct:{ptc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(text):\n",
    "    print('---'*50)\n",
    "    print(f'{text}')\n",
    "    print('---'*50)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Using Scores RMSE values Transformed \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "rf 1.2825887529641644 0.9871741124703584\n",
      "gb 1.4972440834739704 0.9850275591652603\n",
      "br 2.590048627323648 0.9740995137267635\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "model scores as weights\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "wensemble\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "RMSE:1.545752075088763 0.9845424792491124% \n",
      "R2:0.8565092153846584 EVS:0.8565173758793698 MBE:1.1446663183068486 MAD:0.8741299776178408 L90:2.4210025091301297\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mensemble\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "RMSE:1.5484351560925746 0.9845156484390742% \n",
      "R2:0.8560106470314781 EVS:0.8560195186175872 MBE:1.1476986385007468 MAD:0.8732179951977486 L90:2.429270700875538\n"
     ]
    }
   ],
   "source": [
    "context('Using Scores RMSE values Transformed ') # try MAE or my PERC\n",
    "scores = evaluate_models(models, xtrain, xvalid, ytrain, yvalid, True)\n",
    "context('model scores as weights')\n",
    "wensemble = VotingRegressor(estimators=models,weights=scores)\n",
    "mensemble = VotingRegressor(estimators=models)\n",
    "context('wensemble')\n",
    "evaluate_ensemble(wensemble,Xtrain, Xtest, Ytrain, Ytest)\n",
    "context('mensemble')\n",
    "evaluate_ensemble(mensemble,Xtrain, Xtest, Ytrain, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Using Scores RMSE values\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "rf 1.283108930664768 -1.283108930664768\n",
      "gb 1.4972457874684382 -1.4972457874684382\n",
      "br 2.590048627323648 -2.590048627323648\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "model scores as weights\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "wensemble\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "RMSE:1.7391015238017125 0.9826089847619829% \n",
      "R2:0.8183672304254324 EVS:0.8183776814869037 MBE:1.329834773411088 MAD:1.0405360720136976 L90:2.8011201699161568\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mensemble\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "RMSE:1.5481385419879066 0.9845186145801209% \n",
      "R2:0.8560658061799995 EVS:0.8560732629712906 MBE:1.147891428132267 MAD:0.8799009749432152 L90:2.429308700772168\n"
     ]
    }
   ],
   "source": [
    "context('Using Scores RMSE values') # try MAE or my PERC\n",
    "scores = evaluate_models(models, xtrain, xvalid, ytrain, yvalid, False)\n",
    "context('model scores as weights')\n",
    "wensemble = VotingRegressor(estimators=models,weights=scores)\n",
    "mensemble = VotingRegressor(estimators=models)\n",
    "context('wensemble')\n",
    "evaluate_ensemble(wensemble,Xtrain, Xtest, Ytrain, Ytest)\n",
    "context('mensemble')\n",
    "evaluate_ensemble(mensemble,Xtrain, Xtest, Ytrain, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the ensemble to perform better than any contributing ensemble member, and this can be checked directly by evaluating each member model on the full train and test sets independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isnt that because we train the enbsemles in the full data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf 1.2732282826105201 -1.2732282826105201\n",
      "gb 1.5024498266047015 -1.5024498266047015\n",
      "br 2.5714964366190096 -2.5714964366190096\n"
     ]
    }
   ],
   "source": [
    "scores_f = evaluate_models(models,Xtrain, Xtest, Ytrain, Ytest, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF outperfoms both ensemble, but both esembles out performs other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Using Scores RMSE values\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "rf 1.2869847217666575 -1.2869847217666575\n",
      "gb 1.4972322890656196 -1.4972322890656196\n",
      "br 2.590048627323648 -2.590048627323648\n",
      "[-1.2869847217666575, -1.4972322890656196, -2.590048627323648]\n",
      "[3 2 1]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "model scores as weights\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "wensemble\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "RMSE:1.7396046122900521 0.9826039538770994% \n",
      "R2:0.8182621295200745 EVS:0.818273549078219 MBE:1.3300614009273461 MAD:1.0366169251218338 L90:2.799784342714556\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mensemble\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "RMSE:1.5484214852494025 0.9845157851475059% \n",
      "R2:0.8560131895301621 EVS:0.8560217889156585 MBE:1.1477569575617543 MAD:0.8755926325761578 L90:2.425603758926588\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "each model scores\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "rf 1.2762599738071239 -1.2762599738071239\n",
      "gb 1.5024774117247166 -1.5024774117247166\n",
      "br 2.5714964366190096 -2.5714964366190096\n"
     ]
    }
   ],
   "source": [
    "context('Using Scores RMSE values') # try MAE or my PERC\n",
    "scores = evaluate_models(models, xtrain, xvalid, ytrain, yvalid, False)\n",
    "\n",
    "print(scores)\n",
    "ranking = 1 + np.argsort(np.argsort(scores))\n",
    "print(ranking)\n",
    "\n",
    "context('model scores as weights')\n",
    "wkensemble = VotingRegressor(estimators=models,weights=ranking)\n",
    "mensemble = VotingRegressor(estimators=models)\n",
    "context('wensemble')\n",
    "evaluate_ensemble(wensemble,Xtrain, Xtest, Ytrain, Ytest)\n",
    "context('mensemble')\n",
    "evaluate_ensemble(mensemble,Xtrain, Xtest, Ytrain, Ytest)\n",
    "context('each model scores')\n",
    "scores_f = evaluate_models(models,Xtrain, Xtest, Ytrain, Ytest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source:\n",
    "> https://machinelearningmastery.com/weighted-average-ensemble-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:1.739531389747936 0.9826046861025206% \n",
      "R2:0.8182774284319296 EVS:0.8182894546598853 MBE:1.329940634926246 MAD:1.0393677889031743 L90:2.808140673169685\n"
     ]
    }
   ],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:1.5478734267686194 0.9845212657323138% \n",
      "R2:0.8561150987684715 EVS:0.8561236402885903 MBE:1.1472321240396923 MAD:0.8760237121361278 L90:2.421826616278281\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split : violets the spatial assumption , it is random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
